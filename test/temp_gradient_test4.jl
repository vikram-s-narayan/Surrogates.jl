using Surrogates
using Optim
using FiniteDiff
using Zygote

function vector_of_tuples_to_matrix(v)
    #convert training data generated by surrogate sampling into a matrix suitable for GEKPLS
    num_rows = length(v)
    num_cols = length(first(v))
    K = zeros(num_rows, num_cols)
    for row in 1:num_rows
        for col in 1:num_cols
            K[row, col] = v[row][col]
        end
    end
    return K
end

function vector_of_tuples_to_matrix2(v)
    #convert gradients into matrix form
    num_rows = length(v)
    num_cols = length(first(first(v)))
    K = zeros(num_rows, num_cols)
    for row in 1:num_rows
        for col in 1:num_cols
            K[row, col] = v[row][1][col]
        end
    end
    return K
end

function sphere_function(x)
    return sum(x .^ 2)
end

n = 50
d = 3
lb = [-10.0 for i in 1:d]
ub = [10.0 for i in 1:3]
x = sample(n, lb, ub, SobolSample())
X = vector_of_tuples_to_matrix(x)
grads = vector_of_tuples_to_matrix2(Zygote.gradient.(sphere_function, x))
y = reshape(sphere_function.(x), (size(x, 1), 1))
xlimits = hcat(lb, ub)

n_test = 20
x_test = sample(n_test, lb, ub, GoldenSample())
X_test = vector_of_tuples_to_matrix(x_test)
y_true = sphere_function.(x_test)
n_comp = 2
delta_x = 0.0001
extra_points = 2
initial_theta = [0.01 for i in 1:n_comp]

function min_rlfv(theta)
    g = GEKPLS(X, y, grads, n_comp, delta_x, xlimits, extra_points, theta)
    return -g.reduced_likelihood_function_value
end

min_rlfv(initial_theta)

FiniteDiff.finite_difference_gradient(min_rlfv, initial_theta)

function g!(G, theta)
    println("theta: ", theta)
    #G = FiniteDiff.finite_difference_gradient(min_rlfv, theta) #why does this not work?
    fd = FiniteDiff.finite_difference_gradient(min_rlfv, theta)
    G[1] = fd[1]
    G[2] = fd[2]
end

lower = [0.0, 0.0]
upper = [20.0, 20.0]
initial_theta = [0.01, 0.01]
#inner_optimizer = GradientDescent()
#inner_optimizer = LBFGS()
inner_optimizer = ConjugateGradient()

#results = optimize(min_rlfv, g!, lower, upper, initial_theta, Fminbox(inner_optimizer))
#optimal_theta = results.minimizer
#g = GEKPLS(X, y, grads, n_comp, delta_x, xlimits, extra_points, optimal_theta)
#g.reduced_likelihood_function_value

## same test as above but using rmse as a measure to minimize

function rmse(theta)
    g = GEKPLS(X, y, grads, n_comp, delta_x, xlimits, extra_points, theta)
    y_pred = g(X_test)
    n = size(y_pred,1)
    return sqrt(sum((y_pred - y_true).^2)/n)
end

start_time = time()
results2 = optimize(rmse, g!, lower, upper, initial_theta, Fminbox(inner_optimizer), Optim.Options(time_limit = 30.0))

optimal_theta2 = results2.minimizer

g = GEKPLS(X, y, grads, n_comp, delta_x, xlimits, extra_points, optimal_theta2)

y_pred = g(X_test)
n = size(y_pred, 1)
optimal_rmse = sqrt(sum((y_pred - y_true).^2)/n)

g2 = GEKPLS(X, y, grads, n_comp, delta_x, xlimits, extra_points, [0.01, 0.01])
y_pred2 = g2(X_test)
optimal_rmse2 = sqrt(sum((y_pred2 - y_true).^2)/n)